{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a29b1f-12ab-4b27-b6a0-603fad66f232",
   "metadata": {},
   "source": [
    "# Exploration and Preprocessing of VGP database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15ccda-2bc2-4758-b359-a7a77a9b4b88",
   "metadata": {},
   "source": [
    "This notebook preprocessing site level data from the VGP database compilation, conducts associated statistical tests and make some visualizations of data at the study level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9110406-c02f-4dae-b72f-3eb549fb59d3",
   "metadata": {},
   "source": [
    "## Import scientific Python libraries\n",
    "\n",
    "Import scipy python libraries as well as functions written for the project within vgptools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3c105-829b-4bd3-bb48-2326d3e7ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmagpy import ipmag, pmag\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.crs as ccrs\n",
    "from scipy import optimize\n",
    "import seaborn as sns\n",
    "\n",
    "from vgptools.auxiliar import (get_files_in_directory, spherical2cartesian, \n",
    "                               cartesian2spherical, GCD_cartesian, \n",
    "                               print_pole_statistics, test_fishqq, \n",
    "                               statistical_tests, summary_figure,\n",
    "                               invert_polarity, Plot_VgpsAndSites)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affde5a1-9ed8-445f-bbe9-16a780df3d86",
   "metadata": {},
   "source": [
    "## Gather study files\n",
    "\n",
    "We retrieve all the spreadsheet files corresponding to different studies for which site level data are compiled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83b3cb-993a-44e8-a2bd-2c6c68587795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "data_path_VGP = current_path + '/data/vgp_database'\n",
    "files_names = get_files_in_directory(data_path_VGP)\n",
    "\n",
    "xlsx_file_names = [os.path.splitext(os.path.basename(open(file,'r').name))[0] for file in files_names if file.endswith('.xlsx')] \n",
    "paths = [file for file in files_names if file.endswith('.xlsx')] \n",
    "df_files = pd.DataFrame({'path': paths,  'name_xlsx': xlsx_file_names})\n",
    "df_files[['name_xlsx']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819038d-0ec8-416d-8f10-63a0c242129c",
   "metadata": {},
   "source": [
    "## Single study inspection\n",
    "\n",
    "In order to understand what is going on in one single site, we can conduct statistical tests and visualize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bfd7ff-c0a8-4e28-b097-a7f340f03f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efebd7-4673-4713-bdf9-02a05fd64072",
   "metadata": {},
   "source": [
    "#### Separate the *.xlsx file into two different DFs, `df_vgps` and `df_poles`. \n",
    "Note: the the number of lines to be skipped is hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d955a9-db22-48c3-8455-ba0001dd2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasheet (df_files, file_idx):\n",
    "    \"\"\"\n",
    "    Reads in datasheets and splits them into pole and vgp collections to be filtered, collated and compiled.\n",
    "    Input: standard vgp datasheet (as described in datasheet template)\n",
    "    Output: separate dataframes comprised of the study-level poles and site-level vgps extracted from the datasheet\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(df_files['path'][file_idx]) #, skip_blank_lines=True\n",
    "    df_poles = pd.read_excel(df_files['path'][file_idx], \n",
    "                             skiprows = df[df.iloc[:,0]=='Study level data'].index[0]+2,\n",
    "                             nrows  = df[df.isnull().all(1)].index[1] -3)\n",
    "\n",
    "    df_vgps = pd.read_excel(df_files['path'][file_idx], \n",
    "                            skiprows = df[df.iloc[:,0]=='Site level data'].index[0]+2)\n",
    "\n",
    "    #cast columns\n",
    "    df_vgps = df_vgps.astype({'in_study_pole': int,\n",
    "                              \"slat\":float, \"slon\":float, \"dec\":float, \"inc\":float,\n",
    "                              \"VGP_lat\":float, \"VGP_lon\":float\n",
    "                             })\n",
    "    df_poles = df_poles.astype({'N': int,\n",
    "                              \"slat\":float, \"slon\":float, \"dec\":float, \"inc\":float,\n",
    "                              \"Plat\":float, \"Plon\":float})\n",
    "    return (df_poles, df_vgps)\n",
    "\n",
    "df_poles, df_vgps = split_datasheet(df_files, file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbed212-b7a4-41b7-9967-e7e546963ee4",
   "metadata": {},
   "source": [
    "## We then proceed to populate the VGP DataFrame (`df_vgps`) following different criteria\n",
    " 1. In a previous step, we have calculated the site coordinates of all studies in which these coordinates were not reported, but the Dec/Inc and Plat/Plon were. \n",
    " 2. When VGP coordinates are NOT reported in the original manuscript but the dec/inc and location of the sites was, we calculate them, and take them as face value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c630ca-b227-448f-84b6-4018c23bc2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vgps['VGP_lon'] = df_vgps.apply(lambda row: pmag.dia_vgp(row.dec, row.inc, 1, row.slat, row.slon)[0] if (np.isnan(row.VGP_lon) & (~np.isnan(row.dec) & ~np.isnan(row.slat))) else row.VGP_lon, axis =1) \n",
    "# df_vgps['VGP_lat'] = df_vgps.apply(lambda row: pmag.dia_vgp(row.dec, row.inc, 1, row.slat, row.slon)[1] if (np.isnan(row.VGP_lat) & (~np.isnan(row.dec) & ~np.isnan(row.slat))) else row.VGP_lat, axis =1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f849b2-773a-4b2a-9ed4-fad78c764d5f",
   "metadata": {},
   "source": [
    "## Check polarity of VGPs against directions\n",
    " 3. Some sites report the backward VGP (so that the VGPs are in the same hemisphere/closer to the principal component). We proceed to check polarity of VGPs against directions. To do that, we recalculate the vgps from the original Dec/Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c16312-2c79-401a-bae9-687a28dc41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we calculate for the entire dataframe the vgps if there is dec/inc and slat/slon values\n",
    "def recalc_vgps(df_vgps):\n",
    "    df_vgps['VGP_lon_recalc'] = df_vgps.apply(lambda row: pmag.dia_vgp(row.dec, row.inc, 1, row.slat, row.slon)[0], axis =1)\n",
    "    df_vgps['VGP_lat_recalc'] = df_vgps.apply(lambda row: pmag.dia_vgp(row.dec, row.inc, 1, row.slat, row.slon)[1], axis =1)\n",
    "    return df_vgps\n",
    "\n",
    "df_vgps = recalc_vgps(df_vgps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07362f-c132-49b7-b55a-d27db533b17d",
   "metadata": {
    "tags": []
   },
   "source": [
    " 4. Check distance (`df_vgps['GCD_vgps']`) between the reported VGPs and the recalculated from the directions.\n",
    "\n",
    "In this step we fill the column `df_vgps['coherent_vgps']` with the following tags: \n",
    "- 'spurious' if inconsistent combination of site coordinates + dec/inc + vgp data (+- 4 degrees away from the reported or its backward)\n",
    "- 'coherent' if correct\n",
    "- 'inverted' if inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78d0a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_coherence_vgps(df_vgps):\n",
    "\n",
    "    df_vgps['GCD_vgps'] = df_vgps.apply(lambda row: GCD_cartesian(spherical2cartesian([np.radians(row.VGP_lat), np.radians(row.VGP_lon)]), spherical2cartesian([np.radians(row.VGP_lat_recalc), np.radians(row.VGP_lon_recalc)])), axis=1)\n",
    "\n",
    "    # False if Spurious, True if correct, nan if inverted\n",
    "    df_vgps['coherent_vgps'] = df_vgps.apply(lambda row: 'spurious' if (row.GCD_vgps > np.radians(4) and row.GCD_vgps < np.radians(176)) else ('coherent' if row.GCD_vgps < np.radians(4) else 'inverted' if row.GCD_vgps > np.radians(176) else np.nan ), axis =1) #True if it is ok, nan\n",
    "    \n",
    "    return df_vgps\n",
    "df_vgps = check_coherence_vgps(df_vgps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543b4ae-4267-4f07-8ca5-3ea87b9ab0ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Catch some exceptions:\n",
    " - Missing slat/slon and/or dec/in where no vgp is reported\n",
    " - Missing dec/inc and/or vgp where no site coordinates are reported; cannot calculate site locations.\n",
    " - Inconsistent combination of site coordinates + dec/inc + vgp\n",
    " - Recalculated VGP was inverted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose(df_vgps):\n",
    "    \n",
    "    if not df_vgps[(df_vgps['slat'].isna() | df_vgps['slon'].isna())].empty:\n",
    "        print (f\" => Missing slat/slon from sites ('name'): {df_vgps[(df_vgps['slat'].isna() | df_vgps['slon'].isna()) ].name.tolist()}\")\n",
    "        print (f\"\")\n",
    "\n",
    "    if not df_vgps[(df_vgps['dec'].isna() | df_vgps['inc'].isna())].empty:\n",
    "        print (f\" => Missing dec/inc from sites ('name'): {df_vgps[(df_vgps['dec'].isna() | df_vgps['inc'].isna()) ].name.tolist()}\")\n",
    "        print (f\"\")\n",
    "\n",
    "    if not df_vgps[(df_vgps['VGP_lat'].isna() | df_vgps['VGP_lon'].isna())].empty:\n",
    "        print (f\" => Missing reported VGPs from sites ('name'): {df_vgps[(df_vgps['VGP_lat'].isna() | df_vgps['VGP_lon'].isna()) ].name.tolist()}\")\n",
    "        print (f\"\")\n",
    "\n",
    "    if not df_vgps[(df_vgps['dec'].isna() & df_vgps['slat'].isna())].empty:    \n",
    "        print (f\" => Missing slat/slon and/or dec/inc from sites ('name'): {df_vgps[(df_vgps['dec'].isna() & df_vgps['slat'].isna())].name.tolist()} where no vgp is reported; cannot calculate vgp\")\n",
    "        print (f\"\")\n",
    "\n",
    "    if not df_vgps[df_vgps['coherent_vgps'] == 'spurious'].empty:\n",
    "        print (f\" => Inconsistent combination of site coordinates + dec/inc + vgp data from site(s) {df_vgps[df_vgps['coherent_vgps'] == 'spurious'].name.tolist()}\")\n",
    "        print (f\"\")\n",
    "\n",
    "    if not df_vgps[df_vgps['coherent_vgps'] == 'inverted'].empty:\n",
    "        print (f\" => inverted vgp from sites ('name'): {df_vgps[df_vgps['coherent_vgps'] == 'inverted'].name.tolist()}\")\n",
    "        print (f\"\")\n",
    "\n",
    "    if not df_vgps[df_vgps['coherent_vgps'] == 'coherent'].empty:\n",
    "        print (f\" => Coherent dec/inc in sites ('name'): {df_vgps[df_vgps['coherent_vgps'] == 'coherent'].name.tolist()}\")\n",
    "        print (f\"\")\n",
    "        \n",
    "verbose(df_vgps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d27455-ea4f-4096-add9-14901cb920ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summarize site level data\n",
    "\n",
    "For each reported pole from the selected manuscript, we iterate through the constituent site-level data and: \n",
    "1) cast all vgps into a common polarity and re-compute the Fisher mean paleomagnetic pole\n",
    "2) plot the site locations, vgps, and the results of reversal and Fisher distribution tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby iterates through DFs (i) 'grouped by' the column of interest \n",
    "misf1= []\n",
    "misf2= []\n",
    "studies= []\n",
    "pps= []\n",
    "\n",
    "def process_study(df_vgps,file_idx):\n",
    "    for pole, df_pole in df_vgps.groupby('in_study_pole'):\n",
    "\n",
    "        # value represent the index and i represent the DF grouped by the variable of interest  \n",
    "        if pole != 0: #discards vgps discarded by authors\n",
    "\n",
    "            print(f\"==>Analyzing pole {pole} ({df_files.name_xlsx[file_idx]}).\")\n",
    "            print('')\n",
    "            #try:\n",
    "\n",
    "            directions_block = ipmag.make_di_block(df_pole['dec'].tolist(), \n",
    "                                                   df_pole['inc'].tolist(),\n",
    "                                                   unit_vector=False)\n",
    "            di_mode1, di_mode2 = pmag.separate_directions(di_block=directions_block)\n",
    "\n",
    "            vgp_recalc_block = ipmag.make_di_block(df_pole['VGP_lon_recalc'].tolist(), \n",
    "                                                   df_pole['VGP_lat_recalc'].tolist(),\n",
    "                                                   unit_vector=False)\n",
    "            #split recalculated vgp population into polarities             \n",
    "            vgp_mode1, vgp_mode2 = pmag.separate_directions(di_block=vgp_recalc_block)\n",
    "            merged_vgps = invert_polarity(vgp_mode1, vgp_mode2)\n",
    "            vgp_mean_recomputed = ipmag.fisher_mean(di_block = merged_vgps)\n",
    "\n",
    "            # calculate the Fisher mean of reported VGPs\n",
    "            df_pole['VGP_lat'] = np.where(df_pole['VGP_lat'].isna(), df_pole['VGP_lat_recalc'], df_pole['VGP_lat'])\n",
    "            df_pole['VGP_lon'] = np.where(df_pole['VGP_lon'].isna(), df_pole['VGP_lon_recalc'], df_pole['VGP_lon'])\n",
    "            df_pole['vgp_lat_NH'] = np.where(df_pole['VGP_lat'] < 0, -df_pole['VGP_lat'], df_pole['VGP_lat'])\n",
    "            df_pole['vgp_lon_NH'] = np.where(df_pole['VGP_lat'] < 0,(df_pole['VGP_lon'] - 180.) % 360., df_pole['VGP_lon'])             \n",
    "            vgp_mean = ipmag.fisher_mean(dec = df_pole['vgp_lon_NH'].tolist(), inc = df_pole['vgp_lat_NH'].tolist())\n",
    "\n",
    "            #reported pole\n",
    "            reported_pole = df_poles[df_poles['pole'] == pole]\n",
    "\n",
    "            study_number = file_idx\n",
    "            study_folder = './study_summaries' + '/' + str(study_number)\n",
    "            if not os.path.exists(study_folder):\n",
    "                os.mkdir(study_folder)\n",
    "\n",
    "            pole_number = pole\n",
    "            pole_folder = study_folder + '/' + str(pole_number)\n",
    "            if not os.path.exists(pole_folder):\n",
    "                os.mkdir(pole_folder)\n",
    "\n",
    "            pole_summary = print_pole_statistics(reported_pole, \n",
    "                                                 vgp_mean, \n",
    "                                                 vgp_mean_recomputed)\n",
    "\n",
    "            with open(pole_folder + '/pole_summary.tex','w') as tf:\n",
    "                tf.write(pole_summary.to_latex())\n",
    "\n",
    "            stat_test_results = statistical_tests(di_mode1, di_mode2, merged_vgps, \n",
    "                                                  study_number=file_idx, \n",
    "                                                  pole_number=pole, \n",
    "                                                  save_folder=pole_folder)\n",
    "\n",
    "            with open(pole_folder + '/stat_test.tex','w') as tf:\n",
    "                tf.write(stat_test_results.to_latex())\n",
    "\n",
    "            summary_figure(df_pole, vgp_mode1, vgp_mode2, \n",
    "                           reported_pole, vgp_mean, pole_folder)\n",
    "\n",
    "    #         except: #could be expanded!\n",
    "    #             print(f\" DEBUG POLE (index): {file_idx}, in_situ_pole: {pole}\") \n",
    "            \n",
    "            \n",
    "            study_name = str(df_files.name_xlsx[file_idx])\n",
    "            study_name = study_name.replace('_', ' ')\n",
    "            pole_local_folder = './' + str(study_number) + '/' + str(pole_number)\n",
    "            tex_file = open('./study_summaries/SI_study_summary.tex', 'a')\n",
    "            tex_file.write('\\n')\n",
    "            \n",
    "            if pole_number == 1:\n",
    "                tex_file.write('\\section{' + study_name + '}')\n",
    "                tex_file.write('\\n')\n",
    "                tex_file.write('\\subsection{Pole ' + str(pole) + '}')\n",
    "            if pole_number > 1:\n",
    "                tex_file.write('\\subsection{Pole ' + str(pole) + '}')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.write('\\input{' + pole_local_folder + '/pole_summary.tex' + '}')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.write('\\input{' + pole_local_folder + '/stat_test.tex' + '}')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.write('\\\\begin{figure}[H]')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.write('\\centering')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.write('\\includegraphics[width=5 in]{' + pole_local_folder + '/pole_summary.png}')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.write('\\caption{Summary of data from study ' + str(file_idx) + ': ' + study_name + '}')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.write('\\end{figure}')\n",
    "            tex_file.write('\\n')\n",
    "            tex_file.close()\n",
    "   \n",
    "            print('')\n",
    "            plt.pause(10)\n",
    "            \n",
    "process_study(df_vgps,file_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d71616-3570-4e35-8831-2e227b168e8e",
   "metadata": {},
   "source": [
    "# Iterate through all the files to catch exceptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b018bb9-005a-4526-937f-5b8698301efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_idx in range(0,31):\n",
    "    print(f'')\n",
    "    print(f'========================= NEW POLE : {df_files.name_xlsx[file_idx]} ({file_idx}) =======================')\n",
    "    \n",
    "    df_poles, df_vgps = split_datasheet(df_files, file_idx)\n",
    "    df_vgps = recalc_vgps(df_vgps)\n",
    "    df_vgps = check_coherence_vgps(df_vgps)\n",
    "    verbose(df_vgps)\n",
    "    process_study(df_vgps,file_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b51d5d-3471-499a-b73c-e968e112d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "studies, pps, misf, froms = [], [], [], []\n",
    "for file_idx in range(len(df_files) -1):\n",
    "    \n",
    "    print(f'')\n",
    "    print(f'========================= NEW POLE : {df_files.name_xlsx[file_idx]} ({file_idx}) =======================')\n",
    "    \n",
    "    df_poles, df_vgps = split_datasheet(df_files, file_idx)\n",
    "    \n",
    "    if df_vgps.empty: continue\n",
    "        \n",
    "    df_vgps = recalc_vgps(df_vgps)\n",
    "    \n",
    "    df_vgps = check_coherence_vgps(df_vgps)\n",
    "   \n",
    "    verbose(df_vgps)\n",
    "    \n",
    "    #Groupby iterates through DFs (i) 'grouped by' the column of interest \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    for pole, df_pole in df_vgps.groupby('in_study_pole'):\n",
    "\n",
    "        # value represent the index and i represent the DF grouped by the variable of interest  \n",
    "        if pole != 0: #discards vgps discarded by authors\n",
    "\n",
    "            print(f\"==>Analyzing pole {pole} ({df_files.name_xlsx[file_idx]}).\")\n",
    "            try:\n",
    "\n",
    "                #split recalculated vgp population into different polarities by comparison against principal component axis             \n",
    "                vgp_recalc_block = list(zip(df_pole['VGP_lon_recalc'].tolist(), df_pole['VGP_lat_recalc'].tolist()))            \n",
    "                mode1, mode2 = pmag.separate_directions(di_block=vgp_recalc_block)\n",
    "\n",
    "                #reversal test            \n",
    "                reversal_test(mode1, mode2)    \n",
    "\n",
    "                #Bayesian reversal Test\n",
    "                P = bayes_probability_heslop(mode1, mode2)\n",
    "                if P != 9999: print(f\"Bayesian reversal test (Heslop & Roberts, 2018) indicates: {bayes_support(P)}\")            \n",
    "\n",
    "                #invert one polarity population, merge populations and test whether a distribution is Fisherian\n",
    "                merged = invert_polarity(mode1, mode2)\n",
    "                test_fishqq(merged)\n",
    "\n",
    "                # calculate the Fisher mean of reported VGPs to test reproducibility (we first fill the 'reported' if empty)\n",
    "                df_pole['VGP_lat'] = np.where(df_pole['VGP_lat'].isna(), df_pole['VGP_lat_recalc'], df_pole['VGP_lat'])\n",
    "                df_pole['VGP_lon'] = np.where(df_pole['VGP_lon'].isna(), df_pole['VGP_lon_recalc'], df_pole['VGP_lon'])\n",
    "\n",
    "                df_pole['vgp_lat_NH'] = np.where(df_pole['VGP_lat'] < 0, -df_pole['VGP_lat'], df_pole['VGP_lat'])\n",
    "                df_pole['vgp_lon_NH'] = np.where(df_pole['VGP_lat'] < 0,(df_pole['VGP_lon'] - 180.) % 360., df_pole['VGP_lon'])             \n",
    "\n",
    "                vgp_mean_recomputed = ipmag.fisher_mean(di_block = merged)\n",
    "                vgp_mean = ipmag.fisher_mean(dec = df_pole['vgp_lon_NH'].tolist(), inc = df_pole['vgp_lat_NH'].tolist())\n",
    "\n",
    "                #Reported pole\n",
    "                pp = df_poles[df_poles['pole'] == pole]\n",
    "\n",
    "                print_pole_statistics(pp, vgp_mean, vgp_mean_recomputed)\n",
    "\n",
    "                #di_block = ipmag.make_di_block(df_pole['VGP_lon'].tolist(),df_pole['VGP_lat'].tolist())\n",
    "\n",
    "                # Plot sites and directions\n",
    "                Plot_VgpsAndSites(df_pole, pp, vgp_mean, mode1, mode2) \n",
    "                \n",
    "                \n",
    "                studies.append(df_files.name_xlsx[file_idx])\n",
    "                pps.append(pole)\n",
    "                misf.append(pmag.angle([vgp_mean['dec'], vgp_mean['inc']], [pp.iloc[0]['Plon'], pp.iloc[0]['Plat']])[0])\n",
    "                froms.append('reported')\n",
    "                \n",
    "                studies.append(df_files.name_xlsx[file_idx])\n",
    "                pps.append(pole)                \n",
    "                misf.append(pmag.angle([vgp_mean_recomputed['dec'], vgp_mean_recomputed['inc']], [pp.iloc[0]['Plon'], pp.iloc[0]['Plat']])[0])\n",
    "                froms.append('recalculated')\n",
    "                \n",
    "            except: #could be expanded!\n",
    "                print(f\" DEBUG POLE (index): {file_idx}, in_situ_pole: {pole}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb7ee4-7fde-4006-bf45-69b8ae9d0089",
   "metadata": {},
   "outputs": [],
   "source": [
    "studies, pps, misf, froms = [], [], [], []\n",
    "for file_idx in range(len(df_files) -1):\n",
    "    \n",
    "    print(f'')\n",
    "    print(f'========================= NEW POLE : {df_files.name_xlsx[file_idx]} ({file_idx}) =======================')\n",
    "    \n",
    "    df_poles, df_vgps = split_datasheet(df_files, file_idx)\n",
    "    \n",
    "    if df_vgps.empty: continue\n",
    "        \n",
    "    df_vgps = recalc_vgps(df_vgps)\n",
    "    \n",
    "    df_vgps = check_coherence_vgps(df_vgps)\n",
    "   \n",
    "    #verbose(df_vgps)\n",
    "    \n",
    "    #Groupby iterates through DFs (i) 'grouped by' the column of interest \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    for pole, df_pole in df_vgps.groupby('in_study_pole'):\n",
    "\n",
    "        # value represent the index and i represent the DF grouped by the variable of interest  \n",
    "        if pole != 0: #discards vgps discarded by authors\n",
    "\n",
    "            print(f\"==>Analizing pole {pole} ({df_files.name_xlsx[file_idx]}).\")\n",
    "          \n",
    "            try:\n",
    "                #split recalculated vgp population into different polarities by comparison against principal component axis             \n",
    "                vgp_recalc_block = list(zip(df_pole['VGP_lon_recalc'].tolist(), df_pole['VGP_lat_recalc'].tolist()))            \n",
    "                mode1, mode2 = pmag.separate_directions(di_block=vgp_recalc_block)\n",
    "\n",
    "                #reversal test            \n",
    "                # reversal_test(mode1, mode2)    \n",
    "\n",
    "                #Bayesian reversal Test\n",
    "                # P = bayes_probability_heslop(mode1, mode2)\n",
    "                # if P != 9999: print(f\"Bayesian reversal test (Heslop & Roberts, 2018) indicates: {bayes_support(P)}\")            \n",
    "\n",
    "                #invert one polarity population, merge populations and test whether a distribution is Fisherian\n",
    "                merged = invert_polarity(mode1, mode2)\n",
    "                # test_fishqq(merged)\n",
    "\n",
    "                # calculate the Fisher mean of reported VGPs to test reproducibility (we first fill the 'reported' if empty)\n",
    "                df_pole['VGP_lat'] = np.where(df_pole['VGP_lat'].isna(), df_pole['VGP_lat_recalc'], df_pole['VGP_lat'])\n",
    "                df_pole['VGP_lon'] = np.where(df_pole['VGP_lon'].isna(), df_pole['VGP_lon_recalc'], df_pole['VGP_lon'])\n",
    "\n",
    "                df_pole['vgp_lat_NH'] = np.where(df_pole['VGP_lat'] < 0, -df_pole['VGP_lat'], df_pole['VGP_lat'])\n",
    "                df_pole['vgp_lon_NH'] = np.where(df_pole['VGP_lat'] < 0,(df_pole['VGP_lon'] - 180.) % 360., df_pole['VGP_lon'])             \n",
    "\n",
    "                vgp_mean_recomputed = ipmag.fisher_mean(di_block = merged)\n",
    "                vgp_mean = ipmag.fisher_mean(dec = df_pole['vgp_lon_NH'].tolist(), inc = df_pole['vgp_lat_NH'].tolist())\n",
    "\n",
    "                #Reported pole\n",
    "                pp = df_poles[df_poles['pole'] == pole]\n",
    "\n",
    "                #print_pole_statistics(pp, vgp_mean, vgp_mean_recomputed)\n",
    "\n",
    "                #di_block = ipmag.make_di_block(df_pole['VGP_lon'].tolist(),df_pole['VGP_lat'].tolist())\n",
    "\n",
    "                # Plot sites and directions\n",
    "                # Plot_VgpsAndSites(df_pole, pp, vgp_mean, mode1, mode2) \n",
    "\n",
    "                # if not (len(vgp_mean_recomputed)!=0 & len(vgp_mean)!=0): \n",
    "\n",
    "                \n",
    "                misf.append(pmag.angle([vgp_mean['dec'], vgp_mean['inc']], [pp.iloc[0]['Plon'], pp.iloc[0]['Plat']])[0])\n",
    "                froms.append('reported')\n",
    "                studies.append(df_files.name_xlsx[file_idx])\n",
    "                pps.append(pole)\n",
    "                \n",
    "                misf.append(pmag.angle([vgp_mean_recomputed['dec'], vgp_mean_recomputed['inc']], [pp.iloc[0]['Plon'], pp.iloc[0]['Plat']])[0])\n",
    "                studies.append(df_files.name_xlsx[file_idx])\n",
    "                pps.append(pole)                                \n",
    "                froms.append('recalculated')\n",
    "                    \n",
    "            except: #could be expanded!\n",
    "                print(f\" DEBUG POLE (index): {file_idx}, in_situ_pole: {pole}\") \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c05fe-f899-4cf6-b8a0-ed1242bc02cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary= {'Study': studies, 'in_study_pole': pps, 'Misfit': misf, 'From' : froms}\n",
    "df_misfits = pd.DataFrame(dictionary)\n",
    "\n",
    "df_misfits = df_misfits[~df_misfits['Misfit'].isna()]\n",
    "\n",
    "df_misfits['Misfit'] = np.where(df_misfits['Misfit'] > 90, 180 - df_misfits['Misfit'], df_misfits['Misfit'])\n",
    "df_misfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b369d7-f0f7-4435-bb2c-ec2c802d71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(16,8)})\n",
    "ax = sns.barplot(x=\"Study\", y= \"Misfit\", hue=\"in_study_pole\", data=df_misfits[df_misfits['From'] == 'reported'])\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "ax.set(title = \"Distance from the reported Paleopole and the reported VGPs\")\n",
    "#ax.xaxis.get_label().set_fontsize(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d31525-c399-4be0-bc82-7bef300f78b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(16,8)})\n",
    "ax = sns.barplot(x=\"Study\", y= \"Misfit\", hue=\"in_study_pole\", data=df_misfits[df_misfits['From'] == 'recalculated'])\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "ax.set(title = \"Distance from the reported Paleopole and the recalculated VGPs\")\n",
    "#ax.xaxis.get_label().set_fontsize(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b59ee-f1d3-48ab-a731-fb71408ebd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
